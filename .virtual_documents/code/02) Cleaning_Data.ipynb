


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import json
import datetime


pd.set_option('display.max_columns', None)





# Import and join data sets

folder = '../data/'
written = False

#def data_import(folder_path):  --->  uncomment this line to use
    global written
    files = os.listdir(folder_path)
    output_file = '../data/taxi.csv'
    for file in files:
        if len(file) <= 9:
            input_file = os.path.join(folder_path, file)
            try:
                df = pd.read_csv(input_file, encoding='utf-8')
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(input_file, encoding='latin1')
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(input_file, encoding='ISO-8859-1')
                    except Exception as e:
                        print(f"Error reading file {file}: {e}")
                        continue
            if not written:
                df.to_csv(output_file, index=False)
                written = True
            else:
                df.to_csv(output_file, index=False, header=False, mode='a')

data_import(folder)


#Reading in dataframe of 2022 trips
trips = pd.read_csv('../data/taxi.csv')

#Dropping Unnamed: 0 column and checking dtypes
trips = trips.drop(columns=['Unnamed: 0'])


#Checking for df shape and NaNs
print(trips.shape)
trips.isna().sum()


#Turning request_datetime into a datetime object
trips['request_datetime'] = pd.to_datetime(trips['request_datetime'], format='mixed')

#Creating a column of dates to turn into the index
trips['req_index'] = pd.to_datetime(trips['request_datetime']).dt.date

#Creating a column of just the time
trips['req_time'] = pd.to_datetime(trips['request_datetime']).dt.time

#Creating req_date column again so can merge later
trips['req_date'] = pd.to_datetime(trips['request_datetime']).dt.date
trips['req_date'] = pd.to_datetime(trips['req_date'], format='mixed')

#Setting index as date for possible forecasting
trips.set_index('req_index', inplace=True)
trips = trips.sort_index()


#Dropping on_scene_datetime since we only need request and dropoff

trips = trips.drop(columns=['on_scene_datetime'])
trips.isna().sum()                   


#Converting pickup time to datetime and creating a just pickup time column
trips['pickup_datetime'] = pd.to_datetime(trips['pickup_datetime'], format='mixed')
trips['pickup_time'] = pd.to_datetime(trips['pickup_datetime']).dt.time

#Converting dropoff time to datetime
trips['dropoff_datetime'] = pd.to_datetime(trips['dropoff_datetime'], format='mixed')

#Creating a dropoff_date column
trips['dropoff_date'] = pd.to_datetime(trips['dropoff_datetime']).dt.date
trips['dropoff_time'] = pd.to_datetime(trips['dropoff_datetime']).dt.time





#Importing Weather dataframe
weather = pd.read_csv('../data/nyc_weather_2022.csv')

#Dropping columsn I don't need
wth = weather.drop(columns=['name', 'tempmax', 'tempmin', 'feelslikemax',
       'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob',
       'precipcover', 'icon', 'snow', 'snowdepth', 'windgust',
       'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',
       'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'sunrise',
       'sunset', 'moonphase', 'conditions', 'description', 'stations'])

#Converting datetime column to a pandas datetime object
wth['datetime'] = pd.to_datetime(wth['datetime'], format='mixed')


#Converting precipitation column to numerics

#Checking the values
wth['preciptype'].value_counts()

#Checking the NaNs
wth['preciptype'].isna().sum()

#Fillin NaNs with "missing" string in order to ordinal map
wth['preciptype'] = wth['preciptype'].fillna('missing')

#Ordinal Mapping preciptypes/combining
precip = {'missing': 0, 'rain': 1, 'snow': 2, 'rain,snow': 3, 
          'rain,freezingrain,snow': 4, 'rain,freezingrain,snow,ice': 5,
          'rain,freezingrain': 6, 'freezingrain': 6, 'freezingrain,snow,ice': 5, 'rain,snow,ice': 5}

wth['preciptype'] = wth['preciptype'].map(precip)

#Renaming weather datetime column for easy merging with trips df later
wth.rename(columns={'datetime': 'req_date'}, inplace=True)

#Converting that column to datetime object for easy merging with trips df later
wth['req_date'] = pd.to_datetime(wth['req_date'], format='mixed')





#Importing Taxi Zones
zones = pd.read_csv('../data/taxi_zones.csv')

#Dropping columns I don't need
zones = zones.drop(columns=['OBJECTID', 'Shape_Leng', 'Shape_Area'])

#Casting LocationID as float for easy merging with trips df
zones['LocationID'] = zones['LocationID'].astype(float)

#Changing LocationID column name for easy merging with trips df
zones = zones.rename(columns={'LocationID':'PULocationID'})


#Making sure the value counts for trips and zones df line up
trips.PULocationID.value_counts(), zones.PULocationID.value_counts()





#Checking the shape of trips and weather df
trips.shape, wth.shape


#Left merging trips and weather to ensure no trips data is lost
df = pd.merge(
    left=trips.reset_index(),
    right=wth,
    how='left',
    on='req_date'
)

#Checking shape of trips and the new df to ensure no trips data is lost
trips.shape, df.shape


#Left merging df and zones as to not loose any df data and to assign each trip a zone
df = pd.merge(
    left=df,
    right=zones,
    how='left',
    on='PULocationID'
)

#Checking shape to see if we lost data
df.shape


#Checking for missings due to undropped NaNs in wth and zones dataset
df.isna().sum()


#Seeing the 888s, I decided to see what those all are

#Isolating the NaNs of df by zone
na_zone = df[df['zone'].isna()]

#Looking to see what the values are that are missing
na_zone['PULocationID'].value_counts()


#Looking into these more and checking taxi zones online, I have discovered that there are actually no taxi zones 57.0 and 265.0, which resulted in 888 NaNs
zones.iloc[262]


#Getting the index back after it was lost
df = df.reset_index()

#Resetting index to be dates for possible forecasting
df['req_index'] = pd.to_datetime(df['req_index'])
df = df.set_index('req_index')
df = df.drop(columns=['index'])

#Sorting df by date
df = df.sort_index()

#Dropping unnecessary columns
df = df.drop(columns=['request_datetime', 'pickup_datetime', 'dropoff_datetime', 'the_geom'])

#Filling borough NaNs with "missing" string for mapping
df['borough'] = df['borough'].fillna('missing')

#Renaming to keep names and create ordinal map
df = df.rename(columns={'borough': 'borough_name'})

#Ordinal mapping the boroughs
ordinals = {'missing': 0, 'Bronx': 1, 'Manhattan': 2, 'Queens': 3, 
          'Brooklyn': 4, 'Staten Island': 5}

#Creating new column
df['borough'] = df['borough_name'].map(ordinals)


#Ensuring my datetimeobjects are datetime objects and creating new columns

#Setting dropoff date
df['dropoff_date'] = pd.to_datetime(df['dropoff_date'], format='mixed')

#Creating request hour column
#df['req_hour'] = df['req_time'].dt.hour

#Formatting pickup and dropoff to get trip duration
df['pickup_time'] = pd.to_datetime(df['pickup_time'], format='%H:%M:%S').dt.floor('min')
df['dropoff_time'] = pd.to_datetime(df['dropoff_time'], format='%H:%M:%S').dt.floor('min')

# Calculate trip duration
df['trip_duration'] = df['dropoff_time'] - df['pickup_time']

#Converting trip duration timedelta type to seconds
df['trip_duration'] = df['trip_duration'].dt.seconds

#Creating a numeric month column
df['month'] = pd.to_datetime(df['req_date']).dt.month

#Creating a numeric day of month column
df['day'] = pd.to_datetime(df['req_date']).dt.day



df.dtypes


#Creating a driver pay + tips column
df['driver_made'] = df['tips'] + df['driver_pay']

#Creating a request day of week
df['day_of_week'] = df['req_date'].dt.strftime('%A')

#Renaming day column to day_of_month
df = df.rename(columns={'day': 'day_of_month'})

#Converting trip_duration to minutes
df['trip_duration'] = df['trip_duration']/60

# Makine an hour column
df['hour'] = df['req_time'].apply(lambda x: x.hour)

# Making a minute column
df['minute'] = df['req_time'].apply(lambda x: x.minute)


df.columns


#Dropping columns no longer needed

df = df.drop(columns=['PULocationID', 'DOLocationID', 'base_passenger_fare',
        'driver_pay', 'req_time', 'req_date', 'pickup_time', 'dropoff_date', 'dropoff_time', 'borough'])

df.head()


#Discovered in EDA that there are negative values in terms of "driver_made", which are probably typos since drivers aren't losing money on trips
df[df['driver_made'] < 0]

#Changing that now and rexporting
df['driver_made'] = df['driver_made'].abs()

#Checking
df[df['driver_made' < 0]]


#Exporting df
df.to_csv('../data/clean/041324_taxi_recs.csv')
