








# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pickle


from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error

from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor

pd.set_option('display.max_columns', None)


#In order to run the model faster, I had to sample it down to 700,000 instead of nearly 2M
new_df = pd.read_csv('../data/clean/041324_taxi_recs.csv')
new_df = new_df.set_index('req_index')
new_df = new_df.sample(n=700000, random_state=2024)
print(new_df.shape)
new_df.head()


new_df.dtypes


new_df = new_df.sort_index()
new_df.shape





#Attempthing Gradient Boost

#setting X, y
X = new_df.drop(columns=['driver_made', 'tips',	'congestion_surcharge'])
y = new_df['driver_made']

#tts
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2024)

# Instantiating
gb = GradientBoostingRegressor(random_state=2024)
ohe = OneHotEncoder()

cat_var = ['borough_name', 'zone', 'day_of_week']
ct = ColumnTransformer([
    ('ohe', OneHotEncoder(), cat_var)], remainder ='passthrough')

pipe = Pipeline([('ct', ct), ('gb', gb)])

#Fitting
pipe.fit(X_train, y_train)


X_test.shape


pipe.score(X_train, y_train), pipe.score(X_test, y_test)


with open('../Taxis/taxi_model.pkl', 'wb') as f:
    pickle.dump(pipe, f)


#Attempthing Gradient Boost with more data

#Setting X2, y2
X2 = new_df.drop(columns=['driver_made', 'tips'])
y2 = new_df['driver_made']

#Train Test Split
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=2024)

#Instantiating GradientBoost and Preprocessor
gb = GradientBoostingRegressor(random_state=2024)
ohe = OneHotEncoder()

#Creating my column transformer
cat_var = ['borough_name', 'zone', 'day_of_week']
ct = ColumnTransformer([
    ('ohe', OneHotEncoder(), cat_var)], remainder ='passthrough')

#Creating my pipeline
pipe2 = Pipeline([('ct', ct), ('gb', gb)])

#Fitting
pipe2.fit(X2_train, y2_train)


print(X2_train.shape)
print(X2_test.shape)
print(y2_train.shape)
print(y2_test.shape)


#Baseline
print(y2.mean())

#Model 
print(test_preds.mean())


#Predictions
train_preds = pipe2.predict(X2_train)
test_preds = pipe2.predict(X2_test)

# Evaluate the model
train_rmse = np.sqrt(mean_squared_error(y2_train, train_preds))
test_rmse = np.sqrt(mean_squared_error(y2_test, test_preds))

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)



#Getting the scores
pipe2.score(X2_train, y2_train), pipe2.score(X2_test, y2_test)


#Saving the model
with open('../final_model.pkl', 'wb') as a:
    pickle.dump(pipe2, a)





#Importing the models
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor


#Creating a smaller dataset to make it quicker
new_df = new_df.sample(n=1000000)


#Make X, y
X = new_df.drop(columns=['driver_made', 'tips'])
y = new_df['driver_made']

#TTS
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2024)

#Instantiating
ohe = OneHotEncoder()
sc = StandardScaler()
rf = RandomForestRegressor()

cat_var = ['borough_name', 'zone', 'day_of_week']

ct = ColumnTransformer([
    ('ohe', OneHotEncoder(), cat_var)], remainder ='passthrough')

#Building the pipeline
pipe = Pipeline([
    ('ct', ct),
    ('sc', StandardScaler(with_mean=False)),
    ('rf', RandomForestRegressor(n_estimators=200, 
                                 max_depth=30, min_samples_split=.05,
                                max_features='sqrt', n_jobs=4))
     ])

#Fitting the model
pipe.fit(X_train, y_train)

#Looking at r2 scores
pipe.score(X_train, y_train), pipe.score(X_test, y_test)


#Feature engineering/tuning to build pipe2
pipe2 = Pipeline([
    ('ct', ct),
    ('sc', StandardScaler(with_mean=False)),
    ('rf', RandomForestRegressor(n_estimators=250, 
                                 max_depth=30, min_samples_split=300,
                                 max_features='sqrt', n_jobs=4))
     ])

#Fitting
pipe2.fit(X_train, y_train)

#Looking at r2 scores
pipe2.score(X_train, y_train), pipe2.score(X_test, y_test)





#Imports I'll need
from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV
from sklearn import metrics


#Creating a smaller dataset
new_df = new_df.sample(n=500000, random_state=2024)


#Ordinal mapping to get numerics

#Days of week
days = {'Sunday': 1, 'Monday': 2, 'Tuesday': 3, 'Wednesday': 4, 'Thursday': 5, 'Friday': 6, 'Saturday': 7}
new_df['day_of_week'] = new_df['day_of_week'].map(days)

#Boroughs
boroughs = {'Manhattan': 1, 'Brooklyn': 2, 'Queens': 3, 'Bronx': 4, 'Staten Island': 5}
new_df['borough_name'] = new_df['borough_name'].map(boroughs)


#Dummifying taxi zones
new_df = pd.get_dummies(columns=['zone'], data=new_df)


#Setting my X, y
new_df = new_df.dropna()
X = new_df.drop(columns=['driver_made', 'tips'])
y = new_df['driver_made']

#Train-test-split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2024)

#Checking the baseline
y_train.mean()


#Instantiating preprocessor and model
sc = StandardScaler()
lr = LinearRegression()

#Building the pipe
pipe = Pipeline([
    ('sc', StandardScaler()),
    ('lr', LinearRegression())
])

#Fitting the pipe
pipe.fit(X_train, y_train)

#Testing the pipe
pipe.score(X_test, y_test), pipe.score(X_test, y_test)


# More Feature Engineering/ building a pipe with less features

#Creating new X with less features
X2 = new_df.drop(columns=['driver_made', 'congestion_surcharge', 'tips'])
y= new_df['driver_made']

#Perfomring tts
X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=2024)


#Fitting the new tts
pipe.fit(X2_train, y_train)

#Getting Score
pipe.score(X2_test, y_test), pipe.score(X2_test, y_test)



# Trying a LassoCV

# Set up a list of Lasso alphas to check.
l_alphas = np.logspace(-3, 0, 100)

# Cross-validate over our list of Lasso alphas.
lasso_cv = LassoCV(alphas=l_alphas, cv=5, max_iter=10)

sc = StandardScaler()

pipe2 = Pipeline([
    ('sc', StandardScaler()),
    ('lasso_cv', lasso_cv)
])

# Fit model using best ridge alpha!
pipe2.fit(X_train, y_train)

#Checking the scores
pipe2.score(X_test, y_test), pipe2.score(X_test, y_test)


#Trying a RidgeRegression

#Setting alphas
r_alphas = np.logspace(0, 5, 100)

# Cross-validate over our list of ridge alphas.
ridge_cv = RidgeCV(alphas=r_alphas, scoring='r2', cv=5)

sc = StandardScaler()

pipe3 = Pipeline([
    ('sc', StandardScaler()),
    ('ridge_cv', ridge_cv)
])

# Fit model using best ridge alpha!
pipe3.fit(X_train, y_train)

#Getting score
pipe3.score(X_test, y_test), pipe3.score(X_test, y_test)


#Checking the best alpha_
ridge_cv.alpha_





#Importing model
import xgboost as xgb
from xgboost.sklearn import XGBRegressor




#Making X, y
X = new_df.drop(columns=['driver_made', 'tips'])
y = new_df['driver_made']

#Train-Test-Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2024)

#Instantiating preprocessor
ohe = OneHotEncoder() 

#Creating pipeline and ColumnTransformer for preprocessing
cat_var = ['borough_name', 'zone', 'day_of_week']
cat_pipe = Pipeline([
    ('ohe', OneHotEncoder(handle_unknown='ignore'))
])

ct = ColumnTransformer([
    ('cat_pipe', cat_pipe, cat_var)])

#Instantiating XGBoost
xgb = XGBRegressor(n_estimators=250, 
                   max_depth=30, min_samples_split=200,
                   max_features=['sqrt', 'log2'], n_jobs=4, random_state=2024, 
                   enable_categorical=True)

#Building the pipeline of transformers and the model
pipe = Pipeline([
    ('ct', ct),
    ('xgb', xgb)
])

#Fitting the model
pipe.fit(X_train, y_train)


#Checking the r2 scores
pipe.score(X_train, y_train), pipe.score(X_test, y_test)


#Checking RMSE scores

#Predictions
train_preds = pipe.predict(X_train)
test_preds = pipe.predict(X_test)

#Evaluate the model
train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))
test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))

#Printing scores
print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)


#Feature engineering/tuning parameter for XGB2 model

#Instantiatingnew model and params
xgb2 = XGBRegressor(n_estimators=300, 
                   max_depth=20, min_samples_split=200,
                   max_features=['sqrt'], n_jobs=4, random_state=2024, 
                   enable_categorical=True)

#Building a second pipeline
pipe2 = Pipeline([
    ('ct', ct),
    ('xgb2', xgb2)
])

#Fitting to new pipe
pipe2.fit(X_train, y_train)


#Checking XGB2 RMSE scores

#Predictions
train_preds = pipe2.predict(X_train)
test_preds = pipe2.predict(X_test)

# Evaluate the model
train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))
test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

#r2 scores
print(pipe2.score(X_train, y_train), pipe2.score(X_test, y_test))


#Feature engineering/tuning parameter for XGB3/pipe3 model

#Instantiating XGB3 with new params
xgb3 = XGBRegressor(n_estimators=500, 
                   max_depth=10, min_samples_split=200,
                   min_child_weight=1, random_state=2024, 
                   enable_categorical=True)

#Building pipe3
pipe3 = Pipeline([
    ('ct', ct),
    ('xgb3', xgb3)
])

#Fitting to pipe3
pipe3.fit(X_train, y_train)


#Checking XGB2 RMSE scores

#Predictions
train_preds = pipe3.predict(X_train)
test_preds = pipe3.predict(X_test)

# Evaluate the model
train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))
test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

#r2 scores
print(pipe3.score(X_train, y_train), pipe3.score(X_test, y_test))



